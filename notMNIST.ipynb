{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: notMNIST_large.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified .\\notMNIST_large.tar.gz\n",
      "Attempting to download: notMNIST_small.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified .\\notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for .\\notMNIST_large. This may take a while. Please wait.\n",
      "['.\\\\notMNIST_large\\\\A', '.\\\\notMNIST_large\\\\B', '.\\\\notMNIST_large\\\\C', '.\\\\notMNIST_large\\\\D', '.\\\\notMNIST_large\\\\E', '.\\\\notMNIST_large\\\\F', '.\\\\notMNIST_large\\\\G', '.\\\\notMNIST_large\\\\H', '.\\\\notMNIST_large\\\\I', '.\\\\notMNIST_large\\\\J']\n",
      "Extracting data for .\\notMNIST_small. This may take a while. Please wait.\n",
      "['.\\\\notMNIST_small\\\\A', '.\\\\notMNIST_small\\\\B', '.\\\\notMNIST_small\\\\C', '.\\\\notMNIST_small\\\\D', '.\\\\notMNIST_small\\\\E', '.\\\\notMNIST_small\\\\F', '.\\\\notMNIST_small\\\\G', '.\\\\notMNIST_small\\\\H', '.\\\\notMNIST_small\\\\I', '.\\\\notMNIST_small\\\\J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_LARGE/A/ZXVyb2Z1cmVuY2UgYm9sZGl0YWxpYy50dGY=.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABnUlEQVR4nG2SvUvWYRSG7+d5fiYW\nFH04NDRUtCa0GERCLlFDNLi1R9AQiUPQ4BBIX0ONlUFEWw21BIE02B/Q4CKVCUJLUUGoWM8552p4\nhff3+nqvN+e+zpfUq5Sah6vfrytpGzWaJeC0ynbeVWpUHqjp84pGNyww5lNfbkp7P2EQ/BruozZ6\nSQUIxrdCiyapEFC5uQVadKpaEIDxWrnt5XRgGQcMnJWhNjQVvcEIPl8jAj/RLm10g0oY43vWCeNy\nC1p0xiyo3FL+iFdmu5U5HVzBcebKYH5MdRZyF5jeYjhf90m6hAVrR5U7wdmnz1kjxe2hI8RvL/jO\nkaW8CTzrFkBsEBEOVO50Oso69C2cHhnzypJSKXNYr4fzY7gz4QwGXrsKcMYkFV1gc6W9uVNqsh9+\nQkpe3j3KbA7G9PGQRqUy8AHDWR5uHeEe1VncJd2lEvZvTDtK09FgM4FDHdEEFlSmWnvOOrZOOFe0\nRGC86j38wAJuPNdfx/iyP7UPX/Qiqvl7PSNYO9n7TkUXASa1++nq4vmtr5by/T8/Z9J/QGlJVaSN\nDzoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"notMNIST_LARGE/A\"\n",
    "onlyfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "file = folder + \"/\" + onlyfiles[-1]\n",
    "print(file)\n",
    "Image(filename=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling .\\notMNIST_large\\A.pickle.\n",
      ".\\notMNIST_large\\A\n",
      "Could not read: .\\notMNIST_large\\A\\RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file '.\\\\notMNIST_large\\\\A\\\\RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n",
      "Could not read: .\\notMNIST_large\\A\\SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file '.\\\\notMNIST_large\\\\A\\\\SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n",
      "Could not read: .\\notMNIST_large\\A\\Um9tYW5hIEJvbGQucGZi.png : cannot identify image file '.\\\\notMNIST_large\\\\A\\\\Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52909, 28, 28)\n",
      "Mean: -0.12825\n",
      "Standard deviation: 0.443121\n",
      "Pickling .\\notMNIST_large\\B.pickle.\n",
      ".\\notMNIST_large\\B\n",
      "Could not read: .\\notMNIST_large\\B\\TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file '.\\\\notMNIST_large\\\\B\\\\TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.00756303\n",
      "Standard deviation: 0.454491\n",
      "Pickling .\\notMNIST_large\\C.pickle.\n",
      ".\\notMNIST_large\\C\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.142258\n",
      "Standard deviation: 0.439806\n",
      "Pickling .\\notMNIST_large\\D.pickle.\n",
      ".\\notMNIST_large\\D\n",
      "Could not read: .\\notMNIST_large\\D\\VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file '.\\\\notMNIST_large\\\\D\\\\VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.0573678\n",
      "Standard deviation: 0.455648\n",
      "Pickling .\\notMNIST_large\\E.pickle.\n",
      ".\\notMNIST_large\\E\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.069899\n",
      "Standard deviation: 0.452942\n",
      "Pickling .\\notMNIST_large\\F.pickle.\n",
      ".\\notMNIST_large\\F\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.125583\n",
      "Standard deviation: 0.44709\n",
      "Pickling .\\notMNIST_large\\G.pickle.\n",
      ".\\notMNIST_large\\G\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0945814\n",
      "Standard deviation: 0.44624\n",
      "Pickling .\\notMNIST_large\\H.pickle.\n",
      ".\\notMNIST_large\\H\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0685221\n",
      "Standard deviation: 0.454232\n",
      "Pickling .\\notMNIST_large\\I.pickle.\n",
      ".\\notMNIST_large\\I\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: 0.0307862\n",
      "Standard deviation: 0.468899\n",
      "Pickling .\\notMNIST_large\\J.pickle.\n",
      ".\\notMNIST_large\\J\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.153358\n",
      "Standard deviation: 0.443656\n",
      "Pickling .\\notMNIST_small\\A.pickle.\n",
      ".\\notMNIST_small\\A\n",
      "Could not read: .\\notMNIST_small\\A\\RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file '.\\\\notMNIST_small\\\\A\\\\RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.132626\n",
      "Standard deviation: 0.445128\n",
      "Pickling .\\notMNIST_small\\B.pickle.\n",
      ".\\notMNIST_small\\B\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535609\n",
      "Standard deviation: 0.457115\n",
      "Pickling .\\notMNIST_small\\C.pickle.\n",
      ".\\notMNIST_small\\C\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141521\n",
      "Standard deviation: 0.44269\n",
      "Pickling .\\notMNIST_small\\D.pickle.\n",
      ".\\notMNIST_small\\D\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492167\n",
      "Standard deviation: 0.459759\n",
      "Pickling .\\notMNIST_small\\E.pickle.\n",
      ".\\notMNIST_small\\E\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0599148\n",
      "Standard deviation: 0.45735\n",
      "Pickling .\\notMNIST_small\\F.pickle.\n",
      ".\\notMNIST_small\\F\n",
      "Could not read: .\\notMNIST_small\\F\\Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file '.\\\\notMNIST_small\\\\F\\\\Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.118185\n",
      "Standard deviation: 0.452279\n",
      "Pickling .\\notMNIST_small\\G.pickle.\n",
      ".\\notMNIST_small\\G\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0925503\n",
      "Standard deviation: 0.449006\n",
      "Pickling .\\notMNIST_small\\H.pickle.\n",
      ".\\notMNIST_small\\H\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586893\n",
      "Standard deviation: 0.458759\n",
      "Pickling .\\notMNIST_small\\I.pickle.\n",
      ".\\notMNIST_small\\I\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526451\n",
      "Standard deviation: 0.471894\n",
      "Pickling .\\notMNIST_small\\J.pickle.\n",
      ".\\notMNIST_small\\J\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.151689\n",
      "Standard deviation: 0.448014\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\notMNIST_large\\\\A.pickle', '.\\\\notMNIST_large\\\\B.pickle', '.\\\\notMNIST_large\\\\C.pickle', '.\\\\notMNIST_large\\\\D.pickle', '.\\\\notMNIST_large\\\\E.pickle', '.\\\\notMNIST_large\\\\F.pickle', '.\\\\notMNIST_large\\\\G.pickle', '.\\\\notMNIST_large\\\\H.pickle', '.\\\\notMNIST_large\\\\I.pickle', '.\\\\notMNIST_large\\\\J.pickle']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Char a')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24e0ea069e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now going to show some labels\n",
    "%matplotlib inline\n",
    "print(train_datasets)\n",
    "with open(train_datasets[0], 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "plt.imshow(letter_set[0])\n",
    "plt.title(\"Char a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52909\n",
      "52911\n",
      "52912\n",
      "52911\n",
      "52912\n",
      "52912\n",
      "52912\n",
      "52912\n",
      "52912\n",
      "52911\n"
     ]
    }
   ],
   "source": [
    "for train_dataset in train_datasets:\n",
    "    with open(train_dataset, \"rb\") as f:\n",
    "        letter_set = pickle.load(f)\n",
    "    print(len(letter_set))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "\n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "                \n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (187341, 28, 28) (187341,)\n",
      "Validation: (8888, 28, 28) (8888,)\n",
      "Testing: (8620, 28, 28) (8620,)\n"
     ]
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "def clean_dataset(md5_set, dataset, labels):\n",
    "    # checking for dups set\n",
    "    ind = 0\n",
    "    clean_dataset = np.empty_like(dataset)\n",
    "    clean_labels = np.empty_like(labels)\n",
    "\n",
    "    for index, letter_image in enumerate(dataset):       \n",
    "        md5_calc = md5(letter_image).hexdigest()\n",
    "        if md5_calc not in md5_set:\n",
    "            md5_set.add(md5_calc)\n",
    "            clean_dataset[ind] = letter_image\n",
    "            clean_labels[ind]  = labels[index]\n",
    "            ind += 1\n",
    "\n",
    "    clean_dataset = clean_dataset[:ind]\n",
    "    clean_labels  = clean_labels[:ind]\n",
    "    return clean_dataset, clean_labels\n",
    "\n",
    "\n",
    "\n",
    "md5_set = set()\n",
    "train_dataset, train_labels = clean_dataset(md5_set, train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = clean_dataset(md5_set, valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = clean_dataset(md5_set, test_dataset, test_labels)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24e11cbea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_letter(dataset, label):\n",
    "    #function will show name of letter based on label\n",
    "    def letter(i):\n",
    "        return 'ABCDEFGHIJ'[i]\n",
    "    n = np.random.randint(len(dataset[:]))\n",
    "    plt.imshow(dataset[n])\n",
    "    print(letter(labels[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST_sanitized.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 643226366\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 78.1 ms\n",
      "Wall time: 84.1 ms\n",
      "Wall time: 1.39 s\n",
      "overlap test valid: 8620\n",
      "overlap train valid: 187341\n",
      "overlap train test: 187341\n"
     ]
    }
   ],
   "source": [
    "#lets use md5 hasing to measure the overlap between something\n",
    "from hashlib import md5\n",
    "#prepare image hashes\n",
    "%time set_valid_dataset = set([ md5(x).hexdigest() for x in valid_dataset])\n",
    "%time set_test_dataset = set([ md5(x).hexdigest() for x in test_dataset])\n",
    "%time set_train_dataset = set([ md5(x).hexdigest() for x in train_dataset])\n",
    "\n",
    "#measure overlaps and print them\n",
    "overlap_test_valid = set_test_dataset - set_valid_dataset\n",
    "print('overlap test valid: ' + str(len(overlap_test_valid)))\n",
    "\n",
    "overlap_train_valid = set_train_dataset - set_valid_dataset\n",
    "print ('overlap train valid: ' + str(len(overlap_train_valid)))\n",
    "\n",
    "overlap_train_test = set_train_dataset - set_test_dataset\n",
    "print ('overlap train test: ' + str(len(overlap_train_test)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "G:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "G:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "G:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "G:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "G:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "G:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "G:\\Users\\Daniel\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in sample cross validation scores \n",
      "0.396062271062\n",
      "0.404532967033\n",
      "0.655855855856\n",
      "0.513180827887\n",
      "0.798985146463\n",
      "0.789093949628\n",
      "0.809995816901\n",
      "0.802999380262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "pickle_file_san = os.path.join(data_root, 'notMNIST_sanitized.pickle')\n",
    "\n",
    "def check_letter(dataset, labels):\n",
    "    #function will show name of letter based on label\n",
    "    def letter(i):\n",
    "        return 'ABCDEFGHIJ'[i]\n",
    "    n = np.random.randint(len(dataset[:]))\n",
    "    plt.imshow(dataset[n])\n",
    "    print(letter(labels[n]))\n",
    "    \n",
    "#load datasets\n",
    "with open(pickle_file, 'rb') as f:\n",
    "        datasets = pickle.load(f)\n",
    "        \n",
    "with open(pickle_file_san, 'rb') as f:\n",
    "        datasets_sanitized = pickle.load(f)\n",
    "        \n",
    "#here is our regressor which we will reuse\n",
    "logregCV = LogisticRegressionCV()\n",
    "\n",
    "\n",
    "#check_letter(datasets['train_dataset'], datasets['train_labels'])\n",
    "\n",
    "#we first have to flatten our matrices with data to train logreg on them\n",
    "flat_train_dataset_50 = [x.flatten() for x in datasets['train_dataset'][:50]]\n",
    "flat_train_dataset_100 = [x.flatten() for x in datasets['train_dataset'][:100]]\n",
    "flat_train_dataset_1000 = [x.flatten() for x in datasets['train_dataset'][:1000]]\n",
    "flat_train_dataset_5000 = [x.flatten() for x in datasets['train_dataset'][:5000]]\n",
    "flat_train_dataset = [x.flatten() for x in datasets['train_dataset']]\n",
    "\n",
    "flat_train_dataset_50_san = [x.flatten() for x in datasets_sanitized['train_dataset'][:50]]\n",
    "flat_train_dataset_100_san = [x.flatten() for x in datasets_sanitized['train_dataset'][:100]]\n",
    "flat_train_dataset_1000_san = [x.flatten() for x in datasets_sanitized['train_dataset'][:1000]]\n",
    "flat_train_dataset_5000_san = [x.flatten() for x in datasets_sanitized['train_dataset'][:5000]]\n",
    "flat_train_dataset_san = [x.flatten() for x in datasets_sanitized['train_dataset']]\n",
    "\n",
    "#now we are going train our logreg on all these data variants\n",
    "model_50 = logregCV.fit(flat_train_dataset_50, datasets['train_labels'][:50])\n",
    "model_100 = logregCV.fit(flat_train_dataset_100, datasets['train_labels'][:100])\n",
    "model_1000 = logregCV.fit(flat_train_dataset_1000, datasets['train_labels'][:1000])\n",
    "model_5000 = logregCV.fit(flat_train_dataset_5000, datasets['train_labels'][:5000])\n",
    "\n",
    "\n",
    "model_50_sanitized = logregCV.fit(flat_train_dataset_50_san, datasets_sanitized['train_labels'][:50])\n",
    "model_100_sanitized = logregCV.fit(flat_train_dataset_100_san, datasets_sanitized['train_labels'][:100])\n",
    "model_1000_sanitized = logregCV.fit(flat_train_dataset_1000_san, datasets_sanitized['train_labels'][:1000])\n",
    "model_5000_sanitized = logregCV.fit(flat_train_dataset_5000_san, datasets_sanitized['train_labels'][:5000])\n",
    "\n",
    "\n",
    "#calc the scores and compare vs sanitized\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "score_50 = cross_val_score(model_50, flat_train_dataset_50, datasets['train_labels'][:50]).mean()\n",
    "score_100 = cross_val_score(model_100, flat_train_dataset_100, datasets['train_labels'][:100]).mean()\n",
    "score_1000 = cross_val_score(model_1000, flat_train_dataset_1000, datasets['train_labels'][:1000]).mean()\n",
    "score_5000 = cross_val_score(model_5000, flat_train_dataset_5000, datasets['train_labels'][:5000]).mean()\n",
    "\n",
    "score_50_san = cross_val_score(model_50_sanitized, flat_train_dataset_50_san, datasets_sanitized['train_labels'][:50]).mean()\n",
    "score_100_san = cross_val_score(model_100_sanitized, flat_train_dataset_100_san, datasets_sanitized['train_labels'][:100]).mean()\n",
    "score_1000_san = cross_val_score(model_1000_sanitized, flat_train_dataset_1000_san, datasets_sanitized['train_labels'][:1000]).mean()\n",
    "score_5000_san = cross_val_score(model_5000_sanitized, flat_train_dataset_5000_san, datasets_sanitized['train_labels'][:5000]).mean()\n",
    "\n",
    "\n",
    "\n",
    "print('in sample cross validation scores ')\n",
    "print(score_50)\n",
    "print(score_50_san)\n",
    "print(score_100)\n",
    "print(score_100_san)\n",
    "print(score_1000)\n",
    "print(score_1000_san)\n",
    "print(score_5000)\n",
    "print(score_5000_san)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4396\n"
     ]
    }
   ],
   "source": [
    "def count_match(A, B):\n",
    "    c = 0\n",
    "    for i, a in enumerate(A):\n",
    "        if a == B[i]:\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "# Check test\n",
    "flat_test_5000 = [x.flatten() for x in datasets['test_dataset'][:5000]]\n",
    "C = logregCV.predict(flat_test_5000)\n",
    "\n",
    "print(count_match(datasets['test_labels'][:5000], C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
